{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master TIDE - Conférences Python 2021\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "Francis Wolinski\n",
    "\n",
    "&copy; 2021 Yotta Conseil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#no warning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rappel sur les chaînes de caractères\n",
    "\n",
    "En Python, les chaînes de caractères sont des instances de la classe `str`. Ce sont des collections ordonnées immuables de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple\n",
    "s = 'Un exemple de chaîne de caractères'\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une chaîne de caractères doit être vue comme une collection d'octets (bytes) qui représente la chaîne en machine et d'un encodage qui exprime la manière dont les caractères sont codés.\n",
    "\n",
    "La méthode `encode()` permet d'encoder une chaîne en octets selon un encodage particulier, par défaut en `UTF-8`. On obtient un objet de type `bytes`.\n",
    "\n",
    "Python gère tout un ensemble d'encodages: `utf-8` (par défaut en Python 3), `ascii`, `latin-1` ou `ISO-8859-1`, `ISO-8859-15`, `cp1252`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodage en UTF-8\n",
    "s.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodage en latin-1 ou ISO-8859-1\n",
    "s.encode('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decodage bytes --> str\n",
    "b'Un exemple de cha\\xeene de caract\\xe8res'.decode('latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple d'écriture de fichier en UTF-8\n",
    "with open('texte.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(s + '\\n' + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple de lecture de fichier en UTF-8\n",
    "with open('texte.txt', encoding='utf-8') as f:\n",
    "    texte = f.read()\n",
    "texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple de lecture de fichier en latin-1\n",
    "with open('texte.txt', encoding='latin-1') as f:\n",
    "    texte = f.read()\n",
    "texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traitement vectoriel avec `pandas`\n",
    "\n",
    "La librairie `pandas` permet d'effectuer un traitement vectoriel des textes en utilisant l'accesseur `str`.\n",
    "\n",
    "Il existe de nombreuses méthodes de la classe `str` reprises dans `pandas`. Par exemple :\n",
    "- `startswith()` : teste si une chaîne commence par une sous-chaîne\n",
    "- `endswith()` : teste si une chaîne se termine par une sous-chaîne\n",
    "- `contains()` : teste si une chaîne contient un motif (regex)\n",
    "- `extract()` : extrait un ou plusieurs motifs (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement d'un fichier texte dans une Series\n",
    "df = pd.read_csv('imdb_master.csv', usecols=['review'], encoding='latin-1')\n",
    "reviews = df['review']\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sélection\n",
    "reviews.loc[reviews.str.startswith('Very ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sélection\n",
    "reviews.loc[reviews.str.contains('Hitchcock')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sélection\n",
    "reviews.loc[reviews.str.contains('Hitchcock', flags=re.I)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "tab = reviews.str.extract(r'(\\w+) Hitchcock', expand=False)\n",
    "var = tab.dropna()\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mots précèdent Hitchcock\n",
    "var.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignement de textes\n",
    "def search(word):\n",
    "    selection = reviews[reviews.str.contains(word)]\n",
    "    result = selection.apply(lambda s: (' '*25 + s + ' '*25)[s.find(word):s.find(word) + 50 + len(word)])    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('Hitchcock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('Ã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "\n",
    "# replace character encoding mistakes\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¡', 'á'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã ', 'à'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã ', 'à'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¢', 'â'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã\\xa0', 'à'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¥', 'å'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã£', 'ã'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã»', 'â'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã§', 'ç'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã©', 'é'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¨', 'è'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã«', 'ë'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ãª', 'ê'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¯', 'ï'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã®', 'î'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¬', 'ì'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã\\xad', 'í'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã±', 'ñ'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã³', 'ó'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã²', 'ò'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¶', 'ö'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã´', 'ô'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ãµ', 'õ'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã°', 'ð'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¸', 'ø'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ãº', 'ú'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¹', 'ù'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¼', 'ü'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã½', 'ý'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¿', 'ÿ'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã?', 'Æ'))\n",
    "reviews = reviews.apply(lambda x: x.replace('Ã¦', 'æ'))\n",
    "# br\n",
    "reviews = reviews.apply(lambda x: x.replace('<br />', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modèle du sac de mots (bag-of-words)\n",
    "\n",
    "Le modèle du sac de mots est un modèle dans lequel un texte est représenté par un dictionnaire (non ordonné) fréquentiel des mots : nombre d'occurences du mot dans un texte, nombre de documents contenant ce mot pour un corpus.\n",
    "\n",
    "Il est possible au préalable de défléchir les mots, c'est-à-dire de les remplacer par une forme simplifiée : passage en minuscules, lemmatisation (masculin, singulier, infinitif), racinisation (noyau lexical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'A bon chat bon rat'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple\n",
    "c = Counter()\n",
    "c.update('A bon chat bon rat'.split())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texte\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du Counter\n",
    "pattern = r'[A-Za-zÀ-ÿ0-9]+'\n",
    "c = Counter()\n",
    "c.update(re.findall(pattern, reviews[0]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mise en Series\n",
    "s = pd.Series(c)\n",
    "s = s.sort_values(ascending=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tous les textes\n",
    "pattern = r'[A-Za-zÀ-ÿ0-9]+'\n",
    "c = Counter()\n",
    "reviews.apply(lambda x: c.update(re.findall(pattern, x)))\n",
    "vocab = pd.Series(c)\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaire fréquentiel\n",
    "# tous les textes\n",
    "pattern = r'[A-Za-zÀ-ÿ0-9]+'\n",
    "c = Counter()\n",
    "reviews.apply(lambda x: c.update(set(re.findall(pattern, x))))\n",
    "vocab = pd.Series(c)\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words(\"english\")\n",
    "\n",
    "stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élisions en anglais\n",
    "[w for w in stopwords_en if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaire fréquentiel\n",
    "# tous les textes\n",
    "# mots en minuscules\n",
    "# sans stop words\n",
    "pattern = r'[A-Za-zÀ-ÿ0-9]+(?:\\'(?:d|ll|re|s|t|ve))?'\n",
    "c = Counter()\n",
    "reviews.apply(lambda x: c.update(set(re.findall(pattern, x.lower()))))\n",
    "vocab = pd.Series(c)\n",
    "vocab = vocab.drop(stopwords_en, errors='ignore')\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcul de bi-grammes\n",
    "\n",
    "Recherche de cooccurrence de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_doc():\n",
    "    for text in tqdm(reviews[:10_000]):\n",
    "        words = word_tokenize(text.lower())\n",
    "        doc = [word for word in words if not word in stopwords_en]\n",
    "        yield doc\n",
    "\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(get_doc())\n",
    "finder.apply_freq_filter(3)\n",
    "scored = finder.score_ngrams(bgm.likelihood_ratio)\n",
    "\n",
    "# save to pickle\n",
    "import pickle\n",
    "with open('scored.pkl', 'wb') as f:\n",
    "    pickle.dump(scored, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pickle\n",
    "import pickle\n",
    "with open('scored.pkl', 'rb') as f:\n",
    "    scored = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-grammes trouvés\n",
    "scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb de bi-grammes\n",
    "len(scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(word1=None, word2=None, n=10):\n",
    "    i = 0\n",
    "    for (w1, w2), s in scored:\n",
    "        if (word1 is None or (w1.find(word1)!=-1)) \\\n",
    "            and (word2 is None or (w2.find(word2)!=-1)):\n",
    "            print(w1, w2, s)\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('john')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector space model avec scikit-learn TF-IDF\n",
    "\n",
    "En recherche d'information, `TF–IDF`, abréviation de **term frequency–inverse document frequency**, est un indicateur statistique qui cherche à capturer l'importance d'un terme dans un document au sein d'un corpus.\n",
    "\n",
    "Un corpus contient `N` documents. Un terme apparaît dans `n` documents du corpus. pour un document donné de longueur `L`, un terme apparaît `T` fois.\n",
    "\n",
    "`TF–IDF` est le produit de 2 facteurs :\n",
    "- *term frequency* (dans un document donné) :\n",
    "    - $\\frac{T}{L}$\n",
    "- *inverse document frequency* (fréquence d'apparition dans le corpus):\n",
    "    - $\\log(\\frac{N}{n})$\n",
    "    \n",
    "Dans ce modèle, chaque document est représenté par un vecteur normalisé composé du calcul `TF–IDF` pour chaque terme qu'il contient (vector space model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie `scikit-learn` permet de calculer les vecteurs `TF–IDF` pour un corpus de documents.\n",
    "\n",
    "Cette librairie comprend différentes classes traitant des textes:\n",
    "\n",
    "- `CountVectorizer`: implémente la tokenisation et le décompte des occurrences\n",
    "- `TfidfTransformer`: transforme une matrice de comptage en une représentation `TF` ou `TF–IDF`\n",
    "- `TfidfVectorizer`: équivalent à *CountVectorizer* suivi par *TfidfTransformer*\n",
    "\n",
    "Ces classes produisent des **matrices creuses**, c'est-à-dire des matrices pleines de 0 à 99% ou 99.9%, puisque chaque document ne contient que *100* ou *1000* termes et que l'ensemble d'un corpus peut contenir plusieurs *100K* termes.\n",
    "\n",
    "*Nota bene*\n",
    "\n",
    "- Traiter des matrices creuses nécessite une certaine attention car il est générallement pas possible de les transformer en *numpy* arrays ou *pandas* dataframes, à cause des limitations de mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Pour plus d'information</b>\n",
    "<ul>\n",
    "    <li><a href='https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction'>https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 CountVectorizer\n",
    "\n",
    "`CountVectorizer`: implémente la tokenisation des termes et le comptage des occurrences.\n",
    "\n",
    "Il effectue le même calcul que `Counter`. On peut calculer facilement le nombre de fois que chaque terme apparaît dans un document, ou dans tout le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une matrice creuse de *100.000* x *144.342*, c'est-à-dire, 144 milliards de nombres, mais qui ne contient que *13.683.613* éléments:\n",
    "\n",
    "En fait, on a :\n",
    "- 100.000: documents\n",
    "- 144.342: termes\n",
    "- 13.683.613: éléments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densité & sparsité\n",
    "density = X.count_nonzero() / X.shape[0] / X.shape[1]\n",
    "print('density:', density)\n",
    "sparsity = 1.0 - density\n",
    "print('sparsity: {:.3f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory error!\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'utiliser la méthode `get_feature_names()` de `CountVectorizer` qui donne la liste de tous les termes du corpus, dans le même ordre que la matrice creuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termes d'un document donné\n",
    "big_review_id = reviews.str.len().idxmax()\n",
    "\n",
    "# ligne de la matrice avec les termes\n",
    "values = X[big_review_id].toarray()[0]\n",
    "\n",
    "# liste de tous les termes\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# mise en Series\n",
    "vocab = pd.Series(values, index=words)\n",
    "\n",
    "# nombre total de termes et nombre de termes dans le document\n",
    "print(len(vocab), len(vocab[vocab!=0]))\n",
    "vocab = vocab[vocab!=0]\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termes de l'ensemble des documents en sommant selon l'axis 0\n",
    "# somme en colonne par terme\n",
    "values = np.asarray(X.sum(axis=0))[0]\n",
    "\n",
    "# liste de tous les termes\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# mise en Series\n",
    "vocab = pd.Series(values, index=words)\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'éliminer les stop words, les mots rares ou inconnus, on utilise le `TfidfVectorizer` avec des paramètres particuliers. Le résultat est une matrice creuse plus petite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TfidfVectorizer\n",
    "pattern = '[A-Za-zÀ-ÿ0-9]+(?:\\'(?:d|ll|re|s|t|ve))?'\n",
    "vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                             token_pattern=pattern,\n",
    "                             min_df=10,\n",
    "                             stop_words=stopwords_en)\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density & sparsity\n",
    "density = X.count_nonzero()/X.shape[0]/X.shape[1]\n",
    "print('density:', density)\n",
    "sparsity = 1.0 - density\n",
    "print('sparsity: {:.3f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de calculer la somme des `TD-IDF` de chaque terme pour l'ensemble du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termes de tous les documents en sommant selon l'axis 0\n",
    "# somme en colonnes\n",
    "values = np.asarray(X.sum(axis=0))[0]\n",
    "index = vectorizer.get_feature_names()\n",
    "vocab = pd.Series(values, index=index)\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser la vectorisation des documents avec `TF-IDF` pour rechercher les documents similaires à un document donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sélection d'un document dans le corpus\n",
    "review_id = 13473\n",
    "df.loc[review_id, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices des termes présents dans ce document\n",
    "indices = np.nonzero(X[review_id].toarray())[1]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termes correspondants\n",
    "termes = [vectorizer.get_feature_names()[i] for i in indices]\n",
    "termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valeur TF-IDF de chaque terme\n",
    "values = [X[review_id, i] for i in indices]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mise en Series\n",
    "s = pd.Series(values, index=termes)\n",
    "s = s.sort_values(ascending=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver les documents similaires, nous calculons le produit scalaire entre le vecteur de chacun des documents et le vecteur du document sélectionné, on conserve les documents qui produisent les meilleurs scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produit scalaire du document donné avec lui-même\n",
    "np.dot(X[review_id], X[review_id].T).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices des 10 meilleurs documents\n",
    "ids = np.dot(X, X[review_id].T).toarray().T[0].argsort()[-10:]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textes asociés\n",
    "df.loc[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de tout mettre dans une seule fonction qui produit les n meilleurs documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui trouve les revues similaires\n",
    "def find_similar_reviews(review_id, n=5):\n",
    "    # print review\n",
    "    print(reviews.loc[review_id])\n",
    "    print()\n",
    "    # ids des termes de la review\n",
    "    word_ids = np.nonzero(X[review_id].toarray())[1]\n",
    "    # termes de la review\n",
    "    words = [vectorizer.get_feature_names()[word_id] for word_id in word_ids]\n",
    "    # TF-IDF des termes de la review\n",
    "    values = [X[review_id, word_id] for word_id in word_ids]\n",
    "    # mises en Series et print\n",
    "    s = pd.Series(values, index=words)\n",
    "    s = s.nlargest(20)\n",
    "    print(s)\n",
    "    print()\n",
    "    # calcul du produit matriciel entre la matrice et la revue\n",
    "    # on retient du n° 2 au n° n+1\n",
    "    ids = np.dot(X, X[review_id].T).toarray().T[0].argsort()[-n-1:-1]\n",
    "    # collecte des reviews et print\n",
    "    results = df.loc[ids[::-1], 'review']\n",
    "    for i in range(len(results)):\n",
    "        print(i+1)\n",
    "        print(results.iloc[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "find_similar_reviews(review_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random review\n",
    "np.random.seed(1)\n",
    "review_id = np.random.randint(100_000)\n",
    "find_similar_reviews(review_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les autres applications de `TF-IDF` sont la classification, le clustering et l'extraction de thématiques de documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Gensim\n",
    "\n",
    "**Gensim** : topic modelling for humans\n",
    "\n",
    "### 6.1 Word2Vec\n",
    "\n",
    "Hypothèses :\n",
    "- un corpus composé de documents\n",
    "- les documents sont composés de mots\n",
    "\n",
    "L'algorithme utilise le plongement lexical (word embeddings) (i.e. le contexte local) pour produire des vecteurs pour chacun des mots :\n",
    "\n",
    "- préprocessing des documents : `gensim.utils.simple_preprocess()`\n",
    "- modélisation des mots : `gensim.models.Word2Vec()`\n",
    "\n",
    "Ensuite, il est possible d'utiliser les propriétés algébriques des vecteurs pour calculer et utiliser la similarité entre des mots. L'utilisation typique de `gensim` est d'aider à trouver des synonymes ou des antonymes pour des applications dédiées de text mining.\n",
    "\n",
    "- `most_similar(positive=[...])`: trouve les entités similaires à une liste d'entités (synonymes)\n",
    "- `most_similar(positive=[...], negative=[...])`: trouve les entités similaires à une liste d'entités (synonyms) et celles qui sont le moins similaires à une autre (antonymes)\n",
    "- `closer_than(entity1, entity2)`:  trouve les entités plus proches de `entity1` que `entity2` l'est de `entity1`\n",
    "- `doesnt_match()`: trouve l'entité qui correspond le moins au autres\n",
    "\n",
    "Il y a beaucoup d'autres techniques du type \"X2Vec\" : e.g., Doc2Vec, Fact2Vec.\n",
    "\n",
    "Installation, ouvrir un terminal dans le dossier `anaconda3/condabin` :\n",
    "\n",
    "- PC: `.\\conda install -c anaconda gensim`\n",
    "- Mac: `./conda install -c anaconda gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Further reading</b>\n",
    "<ul>\n",
    "    <li><a href='https://radimrehurek.com/gensim/index.html'>https://radimrehurek.com/gensim/index.html</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application de l'algorithme de `gensim `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# preprocesing & modelling\n",
    "if False:\n",
    "    # step 1: preprocessing texts\n",
    "    documents = reviews.apply(lambda x: gensim.utils.simple_preprocess(x)).values\n",
    "    # step 2: process documents\n",
    "    # time depends on vector size, here 150\n",
    "    model = gensim.models.Word2Vec(documents,\n",
    "                              size=150,\n",
    "                              window=10,\n",
    "                              min_count=2,\n",
    "                              workers=4)\n",
    "    model.save('imdb_master.bin')\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load('imdb_master.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation du modèle\n",
    "\n",
    "La fonction `most_similar()` prend des mots positifis et/ou negatifs, afin de trouver des mots similaires, ou non similaires.\n",
    "\n",
    "C'est utile pour fabriquer des liste de synonymes ou d'antonymes pour une application de text mining.\n",
    "\n",
    "**Synonymes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good\n",
    "model.wv.most_similar(positive='good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# great\n",
    "model.wv.most_similar(positive='great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad\n",
    "model.wv.most_similar(positive='bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horrible\n",
    "model.wv.most_similar(positive='horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superhero\n",
    "model.wv.most_similar(positive=['superhero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitchcock\n",
    "model.wv.most_similar(positive=['hitchcock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitchcock & french\n",
    "model.wv.most_similar(positive=['hitchcock', 'french'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmétique des mots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +woman +king -man\n",
    "model.wv.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +man +queen -woman\n",
    "model.wv.most_similar(positive=['queen', 'man'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['actor', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trouver l'intrus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesnt_match\n",
    "model.wv.doesnt_match(['thriller', 'drama', 'comedy', 'melodrama', 'romance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Topic Modelling\n",
    "\n",
    "Le topic modelling est un outil de machine learning non supervisé.\n",
    "\n",
    "Il permet de trouver les thèmes majeurs dans un corpus de textes, de façon automatique (ou presque).\n",
    "La librairie de référence est **gensim**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe plusieurs modèle de Topic Modelling, les plus connus sont :\n",
    "- LSA pour **Latent Semantic Analysis**, qui fonctionne un peu comme une analyse en composantes\n",
    "principales\n",
    "- LDA pour **Latent Dirichlet Allocation**, qui fonctionne un peu comme un K-means\n",
    "\n",
    "Nous allons ici nous focaliser sur la mise en oeuvre de la LDA, mais la méthodologie est la même pour la\n",
    "LSA :\n",
    "- Preprocessing des données textuelles : calcul du BOW et/ou du TF-IDF\n",
    "- Entrainement du modèle (e.g. LDA ou LSA)\n",
    "- Interprétation des résultats et définition des thèmes majeurs\n",
    "- Eventuelles itérations pour affiner les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici nous focaliser sur la mise en oeuvre de la LDA, mais la méthodologie est la même pour la\n",
    "LSA :\n",
    "- Preprocessing des données textuelles : calcul du BOW et/ou du TF-IDF\n",
    "- Entrainement du modèle (e.g. LDA ou LSA)\n",
    "- Interprétation des résultats et définition des thèmes majeurs\n",
    "- Eventuelles itérations pour affiner les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "def clean_data(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stopwords_en]\n",
    "    return tokens\n",
    "\n",
    "# On effectue le traitement en tokens pour la suite\n",
    "tokens = reviews[:10_000].apply(clean_data)\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule de TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords_en, analyzer=lambda x: x)\n",
    "tfidf = vectorizer.fit_transform(tokens)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  6.2.2 Topic Modelling\n",
    "Nous allons ensuite passer à l'étape de Topic Modelling à l'aide de gensim.\n",
    "\n",
    "La première étape est toujours la même : instancier le modèle de LDA qu'il faut importer au préalable de la façon suivante :\n",
    "\n",
    "```python\n",
    "    from gensim.models import LdaModel\n",
    "```\n",
    "\n",
    "La classe `LdaModel` possède la signature suivante (extrait) :\n",
    "\n",
    "```python\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                            id2word,\n",
    "                                            num_topics,\n",
    "                                            random_state,\n",
    "                                            chunksize,\n",
    "                                            passes)\n",
    "```\n",
    "\n",
    "Avec :\n",
    "- `Corpus` le TF-IDF ou le BOW (calculé avec scikit-learn)\n",
    "- `id2word` un dictionnaire avec les correspondances entre indices dans le corpus et les mots (calculé par gensim)\n",
    "- `num_topics` est le nombre désiré de thèmes\n",
    "- `random_state` pour la reproductibilité (mettre toujours la même valeur)\n",
    "- `chunksize` la taille du mini-batch (laisser par défaut)\n",
    "- `passes` le nombre de fois que le modèle verra tout le corpus pour s'entraîner (1 par défaut, en général 5 ou 10 est très bien)\n",
    "\n",
    "La seule information qui manque est le paramètre `id2word`.\n",
    "\n",
    "On peut utiliser pour ça l'objet `Dictionary` du module `gensim.corpora`, qui s'importe de la façon suivante :\n",
    "```python\\n\",\n",
    "    from gensim.corpora import Dictionary\n",
    "```\n",
    "Et qui s'applique directement sur les tokens (d'où la nécessité de le calculer au-dessus) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "id2word = Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dernier détail avant de pouvoir utiliser le modèle, il faut convertir le TF-IDF de `scikit-learn` en un format propre à `gensim` à l'aide de la classe `Sparse2Corpus` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import Sparse2Corpus\n",
    "tfidf_gensim = Sparse2Corpus(tfidf, documents_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, il est possible d'instancier et d'entrainer notre modèle (le tout se faisant en même temps) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# On instancie et entraine un modèle qui trouvera 3 thèmes\n",
    "lda = LdaModel(corpus=tfidf_gensim,\n",
    "    id2word=id2word,\n",
    "    num_topics=3,\n",
    "    random_state=0,\n",
    "    passes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant possible d'afficher les thèmes majeurs, avec la méthode `print_topics()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est ensuite d'interpréter ces résultats...\n",
    "\n",
    "Une possibilité est d'utiliser les vecteurs obtenus par Word2Vec pour calculer un vecteur pour chaque topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtention du topic n° 2\n",
    "topics = lda.show_topics(num_topics=10, num_words=10, log=False, formatted=False)\n",
    "topics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du vecteur pondéré et des mots similaires\n",
    "vector = np.zeros(150)\n",
    "\n",
    "for word, weight in topics[2][1]:\n",
    "    if word in model.wv:\n",
    "        vector += model.wv[word] * weight\n",
    "\n",
    "model.wv.most_similar(positive=[vector], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Visualisation des résultats\n",
    "\n",
    "Il existe un package dédié de visualisation des résultats de LDA, nommé pyLDAvis . Il s'agit du portage en Python du package R `LDAvis`.\n",
    "\n",
    "Il doit au préalable être installé avec la commande suivante :\n",
    "```\n",
    "conda install -c conda-forge pyldavis\n",
    "```\n",
    "\n",
    "L'utilisation du package est ensuite relativement simple, il suffit ensuite d'effectuer quelques imports et d'écrire quelques lignes de code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "bow = [id2word.doc2bow(line) for line in tokens] # convert corpus to BoW format\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda, dictionary=id2word, corpus=bow)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce package fournit une représentation graphique :\n",
    "- à gauche des topics détectés par le modèle LDA, avec leur proximité et le volume de leur\n",
    "vocabulaire,\n",
    "- à droite des termes du corpus et des différents topics, avec leur importance relative dans le topic.\n",
    "\n",
    "Il est possible de sélectionner :\n",
    "- un topic à gauche et de visualiser les termes correspondants,\n",
    "- un terme à droite et de visualiser les topics correspondants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. WordCloud\n",
    "\n",
    "Installation, ouvrir un terminal dans le dossier `anaconda3/condabin` :\n",
    "\n",
    "- sur PC : `.\\conda install -c conda-forge wordcloud`\n",
    "- sur Mac : `./conda install -c conda-forge wordcloud`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud est une librairie Python qui permet de fabriquer des nuages de mots. La taille des mots dépend de leur nombre d'occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import et utilisation de wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# un texte\n",
    "text = reviews.loc[reviews.str.len().idxmax()]\n",
    "\n",
    "# texte aléatoire\n",
    "#from numpy import random\n",
    "#random.seed(100)\n",
    "#text = df.sample(1)['review'].values[0]\n",
    "\n",
    "# à essayer avec différentes options, e.g. stopwords=[]\n",
    "wc = WordCloud(max_words=100,\n",
    "               stopwords=stopwords_en,\n",
    "               normalize_plurals=True,\n",
    "               width=800,\n",
    "               height=600)\n",
    "wc.generate(text)\n",
    "\n",
    "# affichage du nuage de mots\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
